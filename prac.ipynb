{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d7902",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torchaudio.functional import DB_to_amplitude\n",
    "from einops import rearrange\n",
    "from vocos import Vocos\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Helper Functions and Base Class ---\n",
    "\n",
    "def exists(val):\n",
    "    return exists is not None\n",
    "\n",
    "class AudioEncoderDecoder(Module):\n",
    "    pass\n",
    "\n",
    "# --- Your Modified MelVoco Class ---\n",
    "\n",
    "class SpecVoco(AudioEncoderDecoder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        log = True,\n",
    "        sampling_rate = 24000,\n",
    "        n_fft = 1024,\n",
    "        win_length = 640,\n",
    "        hop_length = 160,\n",
    "        pretrained_vocos_path = 'charactr/vocos-mel-24khz'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.log = log\n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "        self.vocos = Vocos.from_pretrained(pretrained_vocos_path)\n",
    "\n",
    "    @property\n",
    "    def downsample_factor(self):\n",
    "        return self.hop_length\n",
    "\n",
    "    @property\n",
    "    def latent_dim(self):\n",
    "        # Number of frequency bins: n_fft // 2 + 1\n",
    "        return self.n_fft // 2 + 1\n",
    "\n",
    "    def encode(self, audio):\n",
    "        stft_transform = T.Spectrogram(\n",
    "            n_fft = self.n_fft,\n",
    "            win_length = self.win_length,\n",
    "            hop_length = self.hop_length,\n",
    "            window_fn = torch.hann_window,\n",
    "            power=2.0 # Ensure the output is a power spectrogram\n",
    "        )\n",
    "\n",
    "        spectrogram = stft_transform(audio)\n",
    "\n",
    "        # Explicitly remove the channel dimension\n",
    "        spectrogram = spectrogram.reshape(spectrogram.shape[0], spectrogram.shape[2], spectrogram.shape[3])\n",
    "\n",
    "        if self.log:\n",
    "            # Apply log conversion (AmplitudeToDB) directly to the raw spectrogram\n",
    "            # AmplitudeToDB expects a power spectrogram when converting to DB\n",
    "            spectrogram = T.AmplitudeToDB()(spectrogram)\n",
    "\n",
    "        # Reshape from (B, Freq, Time) to (B, Time, Freq/Dim)\n",
    "        spectrogram = rearrange(spectrogram, 'b d n -> b n d')\n",
    "        return spectrogram\n",
    "\n",
    "    def decode(self, spectrogram_features):\n",
    "        # Decode method is not needed for visualization, but kept for completeness\n",
    "        spectrogram_features = rearrange(spectrogram_features, 'b n d -> b d n')\n",
    "        # ... (rest of decode logic)\n",
    "        return spectrogram_features # Returning features just for consistency, not actual audio\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# 1. Instantiate the model\n",
    "model = SpecVoco(sampling_rate=16000) # Use 16kHz for common speech audio\n",
    "\n",
    "# 2. Load a sample audio file\n",
    "# IMPORTANT: Replace 'path/to/your/audio.wav' with a path to an audio file you upload or download.\n",
    "# For example, you can upload a file and use its path.\n",
    "try:\n",
    "    # Attempt to load a sample audio file. Torchaudio loads (waveform, sample_rate).\n",
    "    # We take the first channel if it's stereo, and ensure it's a 2D tensor (Batch, Time).\n",
    "    waveform, sr = torchaudio.load('C:\\Users\\ABHILASH\\Desktop\\lucid_Rain\\out.wav')\n",
    "    if sr != model.sampling_rate:\n",
    "         # Resample the audio if the sample rate doesn't match the model's expected rate (recommended)\n",
    "         resampler = T.Resample(sr, model.sampling_rate)\n",
    "         waveform = resampler(waveform)\n",
    "\n",
    "    # Use the first channel and add a batch dimension (1, Time)\n",
    "    audio_input = waveform[:1, :].unsqueeze(0)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"WARNING: Audio file not found. Generating dummy noise for visualization.\")\n",
    "    # Create 3 seconds of dummy noise if file loading fails\n",
    "    T = model.sampling_rate * 3\n",
    "    audio_input = torch.randn(1, 1, T)\n",
    "\n",
    "# 3. Encode to get the Spectrogram features\n",
    "# Output shape: (Batch, Time, Freq)\n",
    "spectrogram_features = model.encode(audio_input)\n",
    "print(f\"\\nEncoded Spectrogram Shape (B, T, Freq): {spectrogram_features.shape}\")\n",
    "\n",
    "# 4. Prepare data for plotting\n",
    "# Convert to NumPy and remove the batch dimension.\n",
    "# We transpose to (Freq, Time) for the plot, which is the standard format for visualization.\n",
    "spectrogram_to_plot = spectrogram_features[0].cpu().numpy().T\n",
    "\n",
    "\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(spectrogram_to_plot, aspect='auto', origin='lower',\n",
    "           interpolation='none', cmap='viridis')\n",
    "\n",
    "# Set labels for verification\n",
    "# X-axis: Time (in frames)\n",
    "plt.xlabel(\"Time Frame Index\")\n",
    "# Y-axis: Frequency Bins\n",
    "# Since Freq Bins = n_fft/2 + 1, this confirms the output is a raw spectrogram\n",
    "plt.ylabel(f\"Frequency Bins (0 to {model.latent_dim - 1})\")\n",
    "plt.title(\"Raw Spectrogram (Log-Magnitude)\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
